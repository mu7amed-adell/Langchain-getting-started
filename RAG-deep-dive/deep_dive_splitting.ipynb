{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97fc726d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df82a29",
   "metadata": {},
   "source": [
    "### Why split documents?  [reference](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/text_splitters.mdx#why-split-documents)\n",
    "There are several reasons to split documents:\n",
    "\n",
    "- Handling non-uniform document lengths: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.\n",
    "- Overcoming model limitations: Many embedding models and language models have maximum input size constraints. - Splitting allows us to process documents that would otherwise exceed these limits.\n",
    "- Improving representation quality: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.\n",
    "- Enhancing retrieval precision: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.\n",
    "- Optimizing computational resources: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.\n",
    "\n",
    "See Greg Kamradt's [chunkviz](https://chunkviz.up.railway.app/) to visualize different splitting strategies discussed below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c498e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42ceec",
   "metadata": {},
   "source": [
    "### What is the goal of chunking?\n",
    "As Greg Kamradt's says in [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb): \n",
    "\"Your goal is not to chunk for chunking sake, our goal is to get our data in a format where it can be retrieved for value later.\"\n",
    "\n",
    "### The Challenge of Chunking\n",
    "Chunking can split a paragraph or a sentence in half, which may cause the text to lose its semantic meaning. This fragmentation can make it harder for retrieval systems to understand and fetch relevant information accurately.\n",
    "\n",
    "### Why is Proper Chunking Important?\n",
    "1. **Semantic Integrity**: Ensures each chunk retains enough context to be meaningful on its own\n",
    "2. **Retrieval Quality**: Affects how well your RAG system can find relevant information\n",
    "3. **Model Performance**: Impacts how effectively LLMs can process and generate responses\n",
    "\n",
    "### Practical Solutions (as shown in later examples):\n",
    "- **Recursive Splitting**: Breaks down text hierarchically (documents → paragraphs → sentences)\n",
    "- **Overlap Strategies**: Maintains context between chunks (as demonstrated with `chunk_overlap=4`)\n",
    "- **Custom Separators**: Uses natural boundaries like `\\n\\n` for paragraphs or spaces for words\n",
    "\n",
    "As we'll see in the following cells, LangChain's text splitters provide these capabilities out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0c948",
   "metadata": {},
   "source": [
    "---\n",
    "### Recursive and Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ae94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "chunk=26\n",
    "overlap=4\n",
    "\n",
    "RecursiveSplitter = RecursiveCharacterTextSplitter(chunk_size=chunk, chunk_overlap=overlap)\n",
    "CharacterSplitter = CharacterTextSplitter(chunk_size=chunk, chunk_overlap=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf5175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveSplitter: ['abcdefghijklmnopqrstuvwxyz']\n",
      "CharacterSplitter: ['abcdefghijklmnopqrstuvwxyz']\n"
     ]
    }
   ],
   "source": [
    "text1 = 'abcdefghijklmnopqrstuvwxyz'\n",
    "print('RecursiveSplitter:', RecursiveSplitter.split_text(text1))\n",
    "print('CharacterSplitter:', CharacterSplitter.split_text(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbf5cf",
   "metadata": {},
   "source": [
    "What is happening? \n",
    "The Recursive splitter is splitting the alphabetics according to the chunk size (26) and the overlap (4), thats why there is no splitting for the 26 characters of the alphabetics.\n",
    "\n",
    "The Character splitter on the other hand, is looking for the default separator to split. The default separator is the (\\n\\n) doubble new line. which in our example text1 does not exist. therefore, it all appear to the splitter as a single chunk.\n",
    "\n",
    "lets try another text. without changing any values just the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26d255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'Mohamed Adel Hassan Ismaiel'\n",
    "len(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9c015",
   "metadata": {},
   "source": [
    "my name is 27 characters long including the spaces, lets try and look how would the splitters handle this simple case. to further understand it more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b98cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveSplitter: ['Mohamed Adel Hassan', 'Ismaiel']\n",
      "CharacterSplitter: ['Mohamed Adel Hassan Ismaiel']\n"
     ]
    }
   ],
   "source": [
    "print('RecursiveSplitter:', RecursiveSplitter.split_text(text2))\n",
    "print('CharacterSplitter:', CharacterSplitter.split_text(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f10773",
   "metadata": {},
   "source": [
    "As I guessed, hope you did too. The Recursive splitter splitted after the 20th character. bacause if the 'ismaiel' was also included it would have exceeded the chunk size of 26. simple!\n",
    "\n",
    "Why doesn't the character splitter split the string?\n",
    "```\n",
    "Character splitter includes /n/n separator by default. Therefore, in this example the text has no new lines. Therefore, it appears to the Character splitter as one single string. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7742baec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveSplitter: ['Mohamed Adel Hassan', 'Ismaiel']\n",
      "CharacterSplitter: ['Mohamed Adel Hassan', 'Ismaiel']\n"
     ]
    }
   ],
   "source": [
    "# lets adjust the separator value to be ' ' a single space.\n",
    "c_split = CharacterTextSplitter(chunk_size=chunk, chunk_overlap= overlap, separator=' ')\n",
    "\n",
    "print('RecursiveSplitter:', RecursiveSplitter.split_text(text2))\n",
    "print('CharacterSplitter:', c_split.split_text(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67652b62",
   "metadata": {},
   "source": [
    "After setting the separator value ' ', they both return the same split. However this is just pure luck because of the text example. How does the RecursiveCharacterTextSplitter actually works?  Recursive Splitter does not use just a single ' ' (space) as its default separator. Instead, it uses a list of separators in order of priority, and it will try to split on the “largest meaningful” one first. \n",
    "```\n",
    "separators = [\n",
    "    \"\\n\\n\",   # paragraph breaks\n",
    "    \"\\n\",    # line breaks\n",
    "    \" \",     # spaces\n",
    "    \"\"       # as a last resort, split by character\n",
    "]\n",
    "```\n",
    "\n",
    "If you think of it for a second, for longer texts, Recursive gives “semantic” splits (paragraphs/lines/words fallback), while Character is rigid (always space / according to the value set for the separator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a06f696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveSplitter: ['Mohamed Adel Hassan', 'Ismaiel', 'Lives in Cairo', 'Works as an AI engineer', 'Enjoys AI, Python, and', 'and teaching']\n",
      "CharacterSplitter: ['Mohamed Adel Hassan Ismaiel\\nLives in Cairo\\nWorks as an AI engineer\\nEnjoys AI, Python, and teaching']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text3 = \"\"\"Mohamed Adel Hassan Ismaiel\n",
    "Lives in Cairo\n",
    "Works as an AI engineer\n",
    "Enjoys AI, Python, and teaching\"\"\"\n",
    "\n",
    "chunk=26\n",
    "overlap=4\n",
    "\n",
    "r_split = RecursiveCharacterTextSplitter(chunk_size=chunk, chunk_overlap=overlap)\n",
    "c_split = CharacterTextSplitter(chunk_size=chunk, chunk_overlap=overlap)\n",
    "\n",
    "print(\"RecursiveSplitter:\", r_split.split_text(text3))\n",
    "print(\"CharacterSplitter:\", c_split.split_text(text3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48221ea5",
   "metadata": {},
   "source": [
    "you may have a question in mind, let me say it out loud. WHY ISN'T THERE ANY OVERLAP???\n",
    "\n",
    "How RecursiveCharacterTextSplitter decides to split\n",
    "\n",
    "It tries separators in order: [\"\\n\\n\", \"\\n\", \" \", \"\"] (paragraph → newline → space → character). It prefers the largest semantic splits first. \n",
    "LangChain\n",
    "\n",
    "If a separator produces pieces that are each ≤ chunk_size, those pieces are used as chunks and the splitter does not break them further — and therefore no overlap is produced between them. Overlap is only visible when a single semantic piece must itself be split into multiple sub-chunks (then those sub-chunks can be made with the requested chunk_overlap). This is the behavior many people stumble over. [Stackoverflow source](https://stackoverflow.com/questions/76681318/why-is-recursivecharactertextsplitter-not-giving-any-chunk-overlap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b98820",
   "metadata": {},
   "source": [
    "### I guess now we understood how everything works at the backend. lets now try out some real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e99e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_text = \"\"\"When reading documents, readers rely on structure to understand the flow of information. \\\n",
    "Headings, subheadings, and paragraphs provide signposts that guide the reader through the material. \\\n",
    "For instance, a heading may introduce a new topic, while a subheading narrows the focus to a detail. \\\n",
    "This organization helps readers follow complex ideas step by step. \\n\\n  \\\n",
    "Formatting cues like bold or italic text also add meaning. \\\n",
    "They emphasize important words or phrases that the writer wants to highlight. \\\n",
    "Lists, whether numbered or bulleted, show relationships between items in a clear manner. \\\n",
    "Altogether, these features create a document that is easier to scan, read, and comprehend.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c0ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0,\n",
    "    separator = ' '\n",
    ")\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] #default values, shown only to make it visible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec57ee5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When reading documents, readers rely on structure to understand the flow of information. Headings, subheadings, and paragraphs provide signposts that guide the reader through the material. For instance, a heading may introduce a new topic, while a subheading narrows the focus to a detail. This organization helps readers follow complex ideas step by step. \\n\\n Formatting cues like bold or italic text also add meaning. They emphasize important words',\n",
       " 'or phrases that the writer wants to highlight. Lists, whether numbered or bulleted, show relationships between items in a clear manner. Altogether, these features create a document that is easier to scan, read, and comprehend.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(more_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7314ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When reading documents, readers rely on structure to understand the flow of information. Headings, subheadings, and paragraphs provide signposts that guide the reader through the material. For instance, a heading may introduce a new topic, while a subheading narrows the focus to a detail. This organization helps readers follow complex ideas step by step.',\n",
       " 'Formatting cues like bold or italic text also add meaning. They emphasize important words or phrases that the writer wants to highlight. Lists, whether numbered or bulleted, show relationships between items in a clear manner. Altogether, these features create a document that is easier to scan, read, and comprehend.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(more_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094408a7",
   "metadata": {},
   "source": [
    "```\n",
    "To finalize, character split is dumb, it splits according to the chunk_size and the seperator if assigned.\n",
    "Recursive split algorithm prioritizes the semantic meaning, think of it like the order of operations in math (PEMDAS). it first applies the \\n\\n which divides according to paragraphs, if the chunk_size is still larger, then it will split on sentences, if still bigger than the specified chunk size, it will split words, so on... \n",
    "\n",
    "This algorithm makes it preserve as much context of each related paragraphs together, avoiding awkward splits within paragraph or sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d364b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('D:\\Langchain\\Langchain-getting-started\\machinelearning-lecture01.pdf')\n",
    "pdf = loader.load()\n",
    "len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9615ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_split = CharacterTextSplitter(chunk_size=450, chunk_overlap=150, separator='\\n')\n",
    "docs = character_split.split_documents(pdf)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0210e",
   "metadata": {},
   "source": [
    "### Let's try out passing the relevant chunks to an LLM. let's see the output when passing the relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c10f3924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "persistent_dir = os.path.join(current_dir, \"db\", \"char_db\")\n",
    "embedding = OllamaEmbeddings(model='nomic-embed-text:v1.5')\n",
    "db = Chroma.from_documents(documents=docs, embedding=embedding, persist_directory=persistent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8fca4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the modules explained in this class ?\"\n",
    "results = db.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "306740bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so\n",
      "\n",
      "Source: D:\\Langchain\\Langchain-getting-started\\machinelearning-lecture01.pdf\n",
      "\n",
      "Document 2:\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so\n",
      "\n",
      "Source: D:\\Langchain\\Langchain-getting-started\\machinelearning-lecture01.pdf\n",
      "\n",
      "Document 3:\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so\n",
      "\n",
      "Source: D:\\Langchain\\Langchain-getting-started\\machinelearning-lecture01.pdf\n",
      "\n",
      "Here are some documents that might help answer the question: What is the modules explained in this class ?\n",
      "\n",
      "Relevant Documents:\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so\n",
      "\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so\n",
      "\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so\n",
      "\n",
      "Please provide an answer based only on the provided documents. Never mention the documents in your response. If the answer is not found in the documents, respond with 'I'm not sure'.\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
    "        \n",
    "combined_user_query = (\n",
    "    \"Here are some documents that might help answer the question: \"\n",
    "    + query\n",
    "    + \"\\n\\nRelevant Documents:\\n\"\n",
    "    + \"\\n\\n\".join([result.page_content for result in results])\n",
    "    + \"\\n\\nPlease provide an answer based only on the provided documents. Never mention the documents in your response. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
    ")\n",
    "print(combined_user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd9e2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, I couldn't find any information about modules mentioned in the provided text. The instructor's introduction appears to be a general welcome and explanation of the class, but it doesn't provide any specific details about the modules that will be covered in the course.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "ollama = ChatOllama(model='llama3.2:3b')\n",
    "## create the messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant named Gulia.\"),\n",
    "    HumanMessage(content=combined_user_query),\n",
    "]\n",
    "result = ollama.invoke(messages)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baad659",
   "metadata": {},
   "source": [
    "### Markdonw Splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65e4491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\"\n",
    "\n",
    "Header_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90c674d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Fun in California', 'Header 2': 'Driving'}, page_content='Try driving on the 1 down to San Diego'),\n",
       " Document(metadata={'Header 1': 'Fun in California', 'Header 2': 'Driving', 'Header 3': 'Food'}, page_content=\"Make sure to eat a burrito while you're there\"),\n",
       " Document(metadata={'Header 1': 'Fun in California', 'Header 2': 'Hiking'}, page_content='Go to Yosemite')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "md_split = MarkdownHeaderTextSplitter(headers_to_split_on=Header_to_split_on)\n",
    "md_split_doc = md_split.split_text(md_text)\n",
    "md_split_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe73a1",
   "metadata": {},
   "source": [
    "- MarkdownHeaderTextSplitter parses a Markdown document.\n",
    "\n",
    "- It splits text based on headers you define in headers_to_split_on.\n",
    "\n",
    "- Each resulting chunk is wrapped in a Document object with:\n",
    "\n",
    "    *   page_content → the text under that section.\n",
    "\n",
    "    *   metadata → a dictionary storing the header hierarchy (e.g., H1, H2, H3).\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- Each Document retains the header hierarchy in metadata.\n",
    "\n",
    "- Useful for structured retrieval → you can search/filter not only by content but also by context (e.g., “all text under Hiking”).\n",
    "\n",
    "- Nested headers get accumulated in the metadata dictionary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a90d0",
   "metadata": {},
   "source": [
    "### TokenTextSplitter\n",
    " breaks text into chunks based on tokens (the units an LLM processes), not words or characters. This ensures chunks align with the model’s tokenizer and stay within token limits.\n",
    "\n",
    "1. Suitable Scenarios\n",
    "\n",
    "- Preparing long documents so each chunk fits inside the model’s token window.\n",
    "\n",
    "- Splitting text before generating embeddings for retrieval (RAG).\n",
    "\n",
    "- Preserving context with controlled token-based overlap between chunks.\n",
    "\n",
    "- When working with texts that include special tokens (e.g., ``), where tokenization accuracy matters.\n",
    "\n",
    "2. When Not to Use\n",
    "\n",
    "- If you need human-friendly splits (e.g., by paragraph, sentence, or section).\n",
    "\n",
    "- When preserving semantic meaning is more important than staying strictly within token boundaries.\n",
    "\n",
    "- Since TokenTextSplitter may cut within a sentence or paragraph, it can break context and reduce readability.\n",
    "\n",
    "\n",
    "\n",
    "We will try the same text we used above, for easier comparison\n",
    "\n",
    "text1, text2, text3, and more_text variables from previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1deaf2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc',\n",
       " 'def',\n",
       " 'gh',\n",
       " 'ij',\n",
       " 'kl',\n",
       " 'mn',\n",
       " 'op',\n",
       " 'q',\n",
       " 'r',\n",
       " 'st',\n",
       " 'uv',\n",
       " 'w',\n",
       " 'xy',\n",
       " 'z']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "token_split_gpt_2 = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
    "token_split_gpt_2.split_text(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c35eb",
   "metadata": {},
   "source": [
    "What is the token splitter doing now? * Note: the default tokenizer in the TokenTextSplitter is gpt-2.\n",
    "\n",
    "How the GPT-2 tokenizer works\n",
    "\n",
    "\n",
    "- GPT-2 uses Byte Pair Encoding (BPE) with a fixed vocabulary (~50,000 tokens).\n",
    "\n",
    "- It always tries to match the longest possible token in the vocabulary.\n",
    "\n",
    "- It starts with a base vocabulary of all single-byte characters (so every lowercase letter \"a\"–\"z\" exists as its own token).\n",
    "\n",
    "- During training, it merges frequent character pairs into new tokens (e.g., \"th\", \"ing\", \"er\", …).\n",
    "\n",
    "So when text is tokenized:\n",
    "\n",
    "- If a string can be matched by a longer token in the vocabulary, it’s grouped together.\n",
    "\n",
    "- If not, it falls back to single characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8962b77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moh', 'amed', ' Ad', 'el', ' Hassan', ' Is', 'm', 'ai', 'el']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_split_gpt_2.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c32f1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moh',\n",
       " 'amed',\n",
       " ' Ad',\n",
       " 'el',\n",
       " ' Hassan',\n",
       " ' Is',\n",
       " 'm',\n",
       " 'ai',\n",
       " 'el',\n",
       " '\\n',\n",
       " 'L',\n",
       " 'ives',\n",
       " ' in',\n",
       " ' Cairo',\n",
       " '\\n',\n",
       " 'Works',\n",
       " ' as',\n",
       " ' an',\n",
       " ' AI',\n",
       " ' engineer',\n",
       " '\\n',\n",
       " 'Enjoy',\n",
       " 's',\n",
       " ' AI',\n",
       " ',',\n",
       " ' Python',\n",
       " ',',\n",
       " ' and',\n",
       " ' teaching']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_split_gpt_2.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cc6fe3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " ' reading',\n",
       " ' documents',\n",
       " ',',\n",
       " ' readers',\n",
       " ' rely',\n",
       " ' on',\n",
       " ' structure',\n",
       " ' to',\n",
       " ' understand',\n",
       " ' the',\n",
       " ' flow',\n",
       " ' of',\n",
       " ' information',\n",
       " '.',\n",
       " ' Head',\n",
       " 'ings',\n",
       " ',',\n",
       " ' sub',\n",
       " 'head',\n",
       " 'ings',\n",
       " ',',\n",
       " ' and',\n",
       " ' paragraphs',\n",
       " ' provide',\n",
       " ' sign',\n",
       " 'posts',\n",
       " ' that',\n",
       " ' guide',\n",
       " ' the',\n",
       " ' reader',\n",
       " ' through',\n",
       " ' the',\n",
       " ' material',\n",
       " '.',\n",
       " ' For',\n",
       " ' instance',\n",
       " ',',\n",
       " ' a',\n",
       " ' heading',\n",
       " ' may',\n",
       " ' introduce',\n",
       " ' a',\n",
       " ' new',\n",
       " ' topic',\n",
       " ',',\n",
       " ' while',\n",
       " ' a',\n",
       " ' sub',\n",
       " 'heading',\n",
       " ' narrow',\n",
       " 's',\n",
       " ' the',\n",
       " ' focus',\n",
       " ' to',\n",
       " ' a',\n",
       " ' detail',\n",
       " '.',\n",
       " ' This',\n",
       " ' organization',\n",
       " ' helps',\n",
       " ' readers',\n",
       " ' follow',\n",
       " ' complex',\n",
       " ' ideas',\n",
       " ' step',\n",
       " ' by',\n",
       " ' step',\n",
       " '.',\n",
       " ' ',\n",
       " '\\n\\n',\n",
       " ' ',\n",
       " ' Format',\n",
       " 'ting',\n",
       " ' cues',\n",
       " ' like',\n",
       " ' bold',\n",
       " ' or',\n",
       " ' ital',\n",
       " 'ic',\n",
       " ' text',\n",
       " ' also',\n",
       " ' add',\n",
       " ' meaning',\n",
       " '.',\n",
       " ' They',\n",
       " ' emphasize',\n",
       " ' important',\n",
       " ' words',\n",
       " ' or',\n",
       " ' phrases',\n",
       " ' that',\n",
       " ' the',\n",
       " ' writer',\n",
       " ' wants',\n",
       " ' to',\n",
       " ' highlight',\n",
       " '.',\n",
       " ' Lists',\n",
       " ',',\n",
       " ' whether',\n",
       " ' numbered',\n",
       " ' or',\n",
       " ' bul',\n",
       " 'leted',\n",
       " ',',\n",
       " ' show',\n",
       " ' relationships',\n",
       " ' between',\n",
       " ' items',\n",
       " ' in',\n",
       " ' a',\n",
       " ' clear',\n",
       " ' manner',\n",
       " '.',\n",
       " ' Alt',\n",
       " 'ogether',\n",
       " ',',\n",
       " ' these',\n",
       " ' features',\n",
       " ' create',\n",
       " ' a',\n",
       " ' document',\n",
       " ' that',\n",
       " ' is',\n",
       " ' easier',\n",
       " ' to',\n",
       " ' scan',\n",
       " ',',\n",
       " ' read',\n",
       " ',',\n",
       " ' and',\n",
       " ' comprehend',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_split_gpt_2.split_text(more_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f46ea",
   "metadata": {},
   "source": [
    "More infor about how tokens correspond to text see this post from OpenAI for more details on how tokens are counted and how they correspond to text.\n",
    "\n",
    "According to the OpenAI post, the approximate token counts for English text are as follows:\n",
    "\n",
    "1 token ~= 4 chars in English\n",
    "1 token ~= ¾ words\n",
    "100 tokens ~= 75 words\n",
    "\n",
    "References\n",
    "\n",
    "- [langchain Tokens](https://python.langchain.com/docs/concepts/tokens/)\n",
    "- [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
