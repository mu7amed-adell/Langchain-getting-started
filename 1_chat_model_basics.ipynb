{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1350dcd2",
   "metadata": {},
   "source": [
    "## LangChain - simple chat with LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843dfd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bb4bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Full response--------\n",
      "content=\"N8N (Node-RED) is an open-source, visual automation platform that allows users to create workflows and integrate different services and APIs. It's designed to be user-friendly and accessible, even for those without extensive programming knowledge.\\n\\nHere are some key features of N8N:\\n\\n1. **Visual Interface**: N8N uses a flow-based interface, where you can connect nodes (functions or actions) in a visual workflow. This makes it easy to understand and manage complex processes.\\n2. **Drag-and-Drop**: The platform offers a drag-and-drop functionality, allowing users to quickly add new nodes and connections without writing code.\\n3. **Node Library**: N8N has an extensive library of pre-built nodes (functions) that can be used for common tasks such as data processing, file management, email sending, and more.\\n4. **Integration with APIs**: N8N supports integration with various APIs and services, including HTTP requests, WebSocket connections, and more.\\n5. **Custom Nodes**: Users can also create custom nodes to extend the platform's functionality or overcome limitations in the existing node library.\\n6. **Cloud-based**: N8N offers a cloud-based service, making it easy to deploy and manage workflows without worrying about infrastructure.\\n\\nSome use cases for N8N include:\\n\\n1. **Automation of repetitive tasks**: Streamline manual processes by automating tasks such as data entry, file management, or email sending.\\n2. **Integration with IoT devices**: Connect sensors, cameras, or other IoT devices to trigger actions or send data to the cloud.\\n3. **Web scraping and monitoring**: Use N8N to scrape data from websites, monitor web servers, or analyze log files.\\n4. **Machine learning and AI**: Integrate machine learning models into workflows using N8N's node library or custom nodes.\\n\\nOverall, N8N is a versatile automation platform that can be used in various industries and use cases, making it an excellent choice for those looking to automate tasks and integrate services.\\n\\nAre you interested in getting started with N8N? I'd be happy to help you set up your first workflow!\" response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-09-09T13:26:17.1662511Z', 'done': True, 'done_reason': 'stop', 'total_duration': 29011525700, 'load_duration': 15229494300, 'prompt_eval_count': 38, 'prompt_eval_duration': 1498786700, 'eval_count': 430, 'eval_duration': 12276644300, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None)} id='run-bb970a86-5fc8-4ea1-b146-7f75aba1af96-0' usage_metadata={'input_tokens': 38, 'output_tokens': 430, 'total_tokens': 468}\n",
      "--------Content only---------\n",
      "N8N (Node-RED) is an open-source, visual automation platform that allows users to create workflows and integrate different services and APIs. It's designed to be user-friendly and accessible, even for those without extensive programming knowledge.\n",
      "\n",
      "Here are some key features of N8N:\n",
      "\n",
      "1. **Visual Interface**: N8N uses a flow-based interface, where you can connect nodes (functions or actions) in a visual workflow. This makes it easy to understand and manage complex processes.\n",
      "2. **Drag-and-Drop**: The platform offers a drag-and-drop functionality, allowing users to quickly add new nodes and connections without writing code.\n",
      "3. **Node Library**: N8N has an extensive library of pre-built nodes (functions) that can be used for common tasks such as data processing, file management, email sending, and more.\n",
      "4. **Integration with APIs**: N8N supports integration with various APIs and services, including HTTP requests, WebSocket connections, and more.\n",
      "5. **Custom Nodes**: Users can also create custom nodes to extend the platform's functionality or overcome limitations in the existing node library.\n",
      "6. **Cloud-based**: N8N offers a cloud-based service, making it easy to deploy and manage workflows without worrying about infrastructure.\n",
      "\n",
      "Some use cases for N8N include:\n",
      "\n",
      "1. **Automation of repetitive tasks**: Streamline manual processes by automating tasks such as data entry, file management, or email sending.\n",
      "2. **Integration with IoT devices**: Connect sensors, cameras, or other IoT devices to trigger actions or send data to the cloud.\n",
      "3. **Web scraping and monitoring**: Use N8N to scrape data from websites, monitor web servers, or analyze log files.\n",
      "4. **Machine learning and AI**: Integrate machine learning models into workflows using N8N's node library or custom nodes.\n",
      "\n",
      "Overall, N8N is a versatile automation platform that can be used in various industries and use cases, making it an excellent choice for those looking to automate tasks and integrate services.\n",
      "\n",
      "Are you interested in getting started with N8N? I'd be happy to help you set up your first workflow!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2:3b\")\n",
    "# Initialize the Ollama chat model\n",
    "# Make sure you have Ollama running locally with a model like llama2 or mistral\n",
    "model = ChatOllama(\n",
    "    model=OLLAMA_MODEL,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Basic conversation\n",
    "messages = [\n",
    "    {\"system\": \"You are a helpful AI assistant.\"},\n",
    "    {\"user\": \"Hello! Can you tell me about LangChain?\"}\n",
    "]\n",
    "\n",
    "# response = chat_model.invoke(messages)\n",
    "response = model.invoke(\"Hello! Can you tell me about n8n automation platform?\")\n",
    "print(\"--------Full response--------\")\n",
    "# Take a look at the full response\n",
    "print(response)\n",
    "print(\"--------Content only---------\")\n",
    "# Take a look at the content only, which is the actual model response.\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf32960",
   "metadata": {},
   "source": [
    "### What have we done??\n",
    "\n",
    "The above code demonstrates a basic interaction with the `ChatOllama` model, which is part of the LangChain framework. Below is a breakdown of the key components:\n",
    "\n",
    " Key Components\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - The `ChatOllama` model is initialized with:\n",
    "     - `model`: The model identifier (e.g., `llama3.2:3b`).\n",
    "     - `temperature`: Controls the randomness of the model's responses (lower values less randmoness | lower values more deterministic).\n",
    "\n",
    "2. **Basic Conversation**:\n",
    "   - A `messages` list is defined to simulate a conversation. It includes:\n",
    "     - A system message (`\"You are a helpful AI assistant.\"`) to set the assistant's behavior.\n",
    "     - A user message (`\"Hello! Can you tell me about LangChain?\"`) to prompt the assistant.\n",
    "\n",
    "3. **Invoking the Model**:\n",
    "   - The `model.invoke()` method is called with a user prompt (`\"Hello! Can you tell me about n8n automation platform?\"`).\n",
    "   - The response is printed in two ways:\n",
    "     - **Full Response**: Includes metadata and other details.\n",
    "     - **Content Only**: Extracts just the model's textual response.\n",
    "\n",
    " Purpose\n",
    "- Configure and initialize a chat model (`ChatOllama`).\n",
    "- Send prompts/query to the model and retrieve responses.\n",
    "- Inspect the model's output difference of response (full llm response) and reponse.content (which is the AI response only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e8cab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "   SystemMessage(content=\"Give a breif summary of the following topics\"),\n",
    "   HumanMessage(content=\"n8n automation platform\"),\n",
    "   AIMessage(content=\"n8n is a free and open-source workflow automation platform that allows you to create workflows that can be used to automate tasks and processes.\"),\n",
    "   HumanMessage(content=\"how does LSTM work in deep learning?\"),\n",
    "]\n",
    "\n",
    "Response = model.invoke(messages)\n",
    "print(Response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b6a86",
   "metadata": {},
   "source": [
    "This snippet demonstrates a conversational interaction with the `ChatOllama` model using a structured `messages` list. Below is a breakdown:\n",
    "\n",
    "#### Key Components\n",
    "1. **Message Structure**:\n",
    "   - `SystemMessage`: Sets the assistant's directive (`\"Give a brief summary of the following topics\"`).\n",
    "   - `HumanMessage`: User queries (e.g., `\"n8n automation platform\"`, `\"how does LSTM work in deep learning?\"`).\n",
    "   - `AIMessage`: A predefined example for the AI response (e.g., explaining `n8n`).\n",
    "\n",
    "2. **Model Invocation**:\n",
    "   - `model.invoke(messages)` processes the conversation.\n",
    "   - `Response.content` prints only the AI's textual output.\n",
    "\n",
    "#### How `langchain_core.messages` Differs\n",
    "- **Dictionary Format (Previous Example)**:\n",
    "  - Messages were structured as dictionaries with `\"system\"`, `\"user\"`, and `\"AI\"` keys.\n",
    "  - Example:\n",
    "    ```python\n",
    "    messages = [\n",
    "        {\"system\": \"You are a helpful AI assistant.\"},\n",
    "        {\"user\": \"Hello! Can you tell me about LangChain?\"}\n",
    "    ]\n",
    "    ```\n",
    "- **Class-Based (Current Example)**:\n",
    "  - Uses dedicated classes (`SystemMessage`, `HumanMessage`, `AIMessage`) from `langchain_core.messages`.\n",
    "  - Benefits:\n",
    "    - **Type Safety**: Explicit message types (e.g., `HumanMessage` for user inputs).\n",
    "    - **Rich Metadata**: Built-in support for additional attributes (e.g., `name`, `id`).\n",
    "    - **Consistency**: Aligns with LangChain's modular design.\n",
    "\n",
    " Purpose\n",
    "  - Structure multi-turn conversations with `SystemMessage`, `HumanMessage`, and `AIMessage`.\n",
    "  - Retrieve and display the AI's response concisely.\n",
    "  - Transition from dictionary-based to class-based message handling.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"), \n",
    "    HumanMessage(content=\"n8n automation platform\"),\n",
    "    AIMessage(content=\"n8n is a free and open-source workflow automation platform that allows you to create workflows that can be used to automate tasks and processes.\"),\n",
    "    HumanMessage(content=\"how does LSTM work in deep learning?\"),\n",
    "    AIMessage(content=\"LSTM is a type of recurrent neural network that is used to process and analyze sequential data, such as time series or natural language. It is particularly effective for tasks that require long-term dependencies, such as language modeling or speech recognition.\")\n",
    "]\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    response = model.invoke(chat_history)\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "    print(f\"AI: {response.content}\")\n",
    "\n",
    "print(\"---- Message History ----\")\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e201f",
   "metadata": {},
   "source": [
    " Now we understand how can we intialize a langchain LLM and how to add System, Human, and a predefined AI Messages example.\n",
    "\n",
    "### Now lets try a Conversational LangChain chat model with a history\n",
    "\n",
    "This code implements an interactive chat system using LangChain's `ChatOllama` model that maintains full conversation history.\n",
    "\n",
    "Key Features\n",
    "\n",
    "1. **Initial Setup**\n",
    "   ```python\n",
    "   chat_history = [\n",
    "       SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "       HumanMessage(content=\"n8n automation platform\"),\n",
    "       AIMessage(content=\"n8n is a free and open-source...\"),\n",
    "       HumanMessage(content=\"how does LSTM work...\"),\n",
    "       AIMessage(content=\"LSTM is a type of recurrent neural network...\")\n",
    "   ]\n",
    "   ```\n",
    "   - Starts with predefined conversation messages, so we can make sure that the LLM can access these past conversations. you can comment all messages and leave the SystemMessage. try it out yourself!\n",
    "   - Includes system prompt and example Q&A pairs.\n",
    "\n",
    "2. **Interactive Loop**\n",
    "   ```python\n",
    "   while True:\n",
    "       query = input(\"You: \")\n",
    "       if query.lower() == \"exit\":\n",
    "           break\n",
    "       chat_history.append(HumanMessage(content=query))\n",
    "       response = model.invoke(chat_history)\n",
    "       chat_history.append(AIMessage(content=response.content))\n",
    "       print(f\"AI: {response.content}\")\n",
    "   ```\n",
    "   - Takes continuous user input\n",
    "   - Appends each exchange to history\n",
    "   - Maintains context across turns\n",
    "\n",
    "3. **History Tracking**\n",
    "   - Full conversation stored in `chat_history` list\n",
    "   - Each turn adds:\n",
    "     - User's `HumanMessage`\n",
    "     - AI's `AIMessage` response\n",
    "\n",
    "4. **Final Output**\n",
    "   ```python\n",
    "   print(\"---- Message History ----\")\n",
    "   print(chat_history)\n",
    "   ```\n",
    "   - Prints complete conversation at end\n",
    "   - Useful for debugging/analysis\n",
    " \n",
    " How It Works\n",
    "\n",
    "1. The model receives the entire message history each time\n",
    "2. This provides context for more coherent responses\n",
    "3. History grows dynamically during conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2febf",
   "metadata": {},
   "source": [
    "---\n",
    "### Switching LLM Providers in LangChain\n",
    "\n",
    "LangChain makes it simple to switch between different LLM providers. Below are examples using OpenAI and Anthropic instead of Ollama.\n",
    "\n",
    "### 1. OpenAI Provider Example\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Initialize OpenAI chat model\n",
    "model = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# this is it, no need to change anything else.\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Uses `ChatOpenAI` class\n",
    "- Requires `OPENAI_API_KEY` environment variable\n",
    "- Specify model version (e.g., \"gpt-4\")\n",
    "- Same message format as Ollama example\n",
    "\n",
    "### 2. Anthropic Provider Example\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "import os\n",
    "\n",
    "# Set your Anthropic API key\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Initialize Anthropic chat model\n",
    "model = ChatAnthropic(model=\"claude-2\", temperature=0.7)\n",
    "\n",
    "# this is it, no need to change anything else.\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Uses `ChatAnthropic` class\n",
    "- Requires `ANTHROPIC_API_KEY` environment variable\n",
    "- Specify model version (e.g., \"claude-2\")\n",
    "- Identical interface to OpenAI/Ollama versions\n",
    "\n",
    " Comparison Table\n",
    "\n",
    "| Provider  | Class           | Environment Variable    | Example Models     |\n",
    "|-----------|-----------------|-------------------------|--------------------|\n",
    "| OpenAI    | `ChatOpenAI`    | `OPENAI_API_KEY`        | gpt-4, gpt-3.5-turbo |\n",
    "| Anthropic | `ChatAnthropic` | `ANTHROPIC_API_KEY`     | claude-2           |\n",
    "| Ollama    | `ChatOllama`    | (None - local only)     | llama2, mistral    |\n",
    "\n",
    " Key Benefits\n",
    "1. **Consistent Interface**: Same `messages` format and `invoke()` method across providers\n",
    "2. **Easy Switching**: Just change the import and initialization\n",
    "4. **Parameter Consistency**: `temperature` and other parameters work the same way. However, some models can have more/less parameter.\n",
    "\n",
    "- Make sure you install the providers packages.\n",
    "- Set appropriate API keys as environment variables.\n",
    "\n",
    "## Notes\n",
    "- Always check provider documentation for:\n",
    "  - Latest model names\n",
    "  - API key requirements\n",
    "  - Rate limits\n",
    "- Consider cost differences between providers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
