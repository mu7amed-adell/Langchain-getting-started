{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1350dcd2",
   "metadata": {},
   "source": [
    "## LangChain - simple chat with LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843dfd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bb4bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Full response--------\n",
      "content=\"N8N (Node-RED) is an open-source automation platform that allows users to create custom workflows by connecting different nodes or components together. It's often used for automating tasks, integrating different services, and building IoT projects.\\n\\nHere are some key features of N8N:\\n\\n1. **Visual workflow editor**: N8N uses a visual interface where you can drag and drop nodes into a flowchart-like editor to create your workflows.\\n2. **Node-based architecture**: The platform is built around a node-based architecture, where each node represents a specific action or function that can be executed in the workflow.\\n3. **Support for multiple protocols**: N8N supports various protocols such as HTTP, MQTT, TCP, UDP, and more, making it easy to integrate with different services.\\n4. **Extensive library of nodes**: The N8N community provides an extensive library of pre-built nodes that can be used to perform common tasks such as authentication, data processing, and file handling.\\n5. **Modular architecture**: N8N is designed to be highly modular, allowing users to create custom nodes or extend the existing ones using Node.js code.\\n6. **Scalability**: N8N is built on top of Node.js and can handle a large volume of workflows, making it suitable for big data processing and IoT applications.\\n\\nSome common use cases for N8N include:\\n\\n1. **Automation of tasks**: N8N can be used to automate repetitive tasks such as sending emails, creating reports, or updating databases.\\n2. **Integration with IoT devices**: N8N's support for various protocols makes it an excellent choice for integrating IoT devices and sensors into a workflow.\\n3. **Data processing**: N8N can be used to process large datasets by connecting different nodes together to perform data transformations, filtering, and analysis.\\n4. **API management**: N8N provides features such as API key management, rate limiting, and caching to help manage APIs.\\n\\nOverall, N8N is a powerful automation platform that allows users to create custom workflows and integrate different services using a visual interface and node-based architecture.\\n\\nHave you worked with N8N before or are you considering it for a project? I'd be happy to help with any specific questions!\" response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-09-09T16:07:48.7033224Z', 'done': True, 'done_reason': 'stop', 'total_duration': 21431802000, 'load_duration': 6881178700, 'prompt_eval_count': 38, 'prompt_eval_duration': 1505801900, 'eval_count': 457, 'eval_duration': 13043351400, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None)} id='run-8c8e079f-54dc-4dd5-a740-2b71a293e15c-0' usage_metadata={'input_tokens': 38, 'output_tokens': 457, 'total_tokens': 495}\n",
      "--------Content only---------\n",
      "N8N (Node-RED) is an open-source automation platform that allows users to create custom workflows by connecting different nodes or components together. It's often used for automating tasks, integrating different services, and building IoT projects.\n",
      "\n",
      "Here are some key features of N8N:\n",
      "\n",
      "1. **Visual workflow editor**: N8N uses a visual interface where you can drag and drop nodes into a flowchart-like editor to create your workflows.\n",
      "2. **Node-based architecture**: The platform is built around a node-based architecture, where each node represents a specific action or function that can be executed in the workflow.\n",
      "3. **Support for multiple protocols**: N8N supports various protocols such as HTTP, MQTT, TCP, UDP, and more, making it easy to integrate with different services.\n",
      "4. **Extensive library of nodes**: The N8N community provides an extensive library of pre-built nodes that can be used to perform common tasks such as authentication, data processing, and file handling.\n",
      "5. **Modular architecture**: N8N is designed to be highly modular, allowing users to create custom nodes or extend the existing ones using Node.js code.\n",
      "6. **Scalability**: N8N is built on top of Node.js and can handle a large volume of workflows, making it suitable for big data processing and IoT applications.\n",
      "\n",
      "Some common use cases for N8N include:\n",
      "\n",
      "1. **Automation of tasks**: N8N can be used to automate repetitive tasks such as sending emails, creating reports, or updating databases.\n",
      "2. **Integration with IoT devices**: N8N's support for various protocols makes it an excellent choice for integrating IoT devices and sensors into a workflow.\n",
      "3. **Data processing**: N8N can be used to process large datasets by connecting different nodes together to perform data transformations, filtering, and analysis.\n",
      "4. **API management**: N8N provides features such as API key management, rate limiting, and caching to help manage APIs.\n",
      "\n",
      "Overall, N8N is a powerful automation platform that allows users to create custom workflows and integrate different services using a visual interface and node-based architecture.\n",
      "\n",
      "Have you worked with N8N before or are you considering it for a project? I'd be happy to help with any specific questions!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2:3b\")\n",
    "# Initialize the Ollama chat model\n",
    "# Make sure you have Ollama running locally with a model like llama2 or mistral\n",
    "model = ChatOllama(\n",
    "    model=OLLAMA_MODEL,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Basic conversation\n",
    "messages = [\n",
    "    {\"system\": \"You are a helpful AI assistant.\"},\n",
    "    {\"user\": \"Hello! Can you tell me about LangChain?\"}\n",
    "]\n",
    "\n",
    "# response = chat_model.invoke(messages)\n",
    "response = model.invoke(\"Hello! Can you tell me about n8n automation platform?\")\n",
    "print(\"--------Full response--------\")\n",
    "# Take a look at the full response\n",
    "print(response)\n",
    "print(\"--------Content only---------\")\n",
    "# Take a look at the content only, which is the actual model response.\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf32960",
   "metadata": {},
   "source": [
    "### What have we done??\n",
    "\n",
    "The above code demonstrates a basic interaction with the `ChatOllama` model, which is part of the LangChain framework. Below is a breakdown of the key components:\n",
    "\n",
    " Key Components\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - The `ChatOllama` model is initialized with:\n",
    "     - `model`: The model identifier (e.g., `llama3.2:3b`).\n",
    "     - `temperature`: Controls the randomness of the model's responses (lower values less randmoness | lower values more deterministic).\n",
    "\n",
    "2. **Basic Conversation**:\n",
    "   - A `messages` list is defined to simulate a conversation. It includes:\n",
    "     - A system message (`\"You are a helpful AI assistant.\"`) to set the assistant's behavior.\n",
    "     - A user message (`\"Hello! Can you tell me about LangChain?\"`) to prompt the assistant.\n",
    "\n",
    "3. **Invoking the Model**:\n",
    "   - The `model.invoke()` method is called with a user prompt (`\"Hello! Can you tell me about n8n automation platform?\"`).\n",
    "   - The response is printed in two ways:\n",
    "     - **Full Response**: Includes metadata and other details.\n",
    "     - **Content Only**: Extracts just the model's textual response.\n",
    "\n",
    " Purpose\n",
    "- Configure and initialize a chat model (`ChatOllama`).\n",
    "- Send prompts/query to the model and retrieve responses.\n",
    "- Inspect the model's output difference of response (full llm response) and reponse.content (which is the AI response only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e8cab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "   SystemMessage(content=\"Give a breif summary of the following topics\"),\n",
    "   HumanMessage(content=\"n8n automation platform\"),\n",
    "   AIMessage(content=\"n8n is a free and open-source workflow automation platform that allows you to create workflows that can be used to automate tasks and processes.\"),\n",
    "   HumanMessage(content=\"how does LSTM work in deep learning?\"),\n",
    "]\n",
    "\n",
    "Response = model.invoke(messages)\n",
    "print(Response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b6a86",
   "metadata": {},
   "source": [
    "This snippet demonstrates a conversational interaction with the `ChatOllama` model using a structured `messages` list. Below is a breakdown:\n",
    "\n",
    "#### Key Components\n",
    "1. **Message Structure**:\n",
    "   - `SystemMessage`: Sets the assistant's directive (`\"Give a brief summary of the following topics\"`).\n",
    "   - `HumanMessage`: User queries (e.g., `\"n8n automation platform\"`, `\"how does LSTM work in deep learning?\"`).\n",
    "   - `AIMessage`: A predefined example for the AI response (e.g., explaining `n8n`).\n",
    "\n",
    "2. **Model Invocation**:\n",
    "   - `model.invoke(messages)` processes the conversation.\n",
    "   - `Response.content` prints only the AI's textual output.\n",
    "\n",
    "#### How `langchain_core.messages` Differs\n",
    "- **Dictionary Format (Previous Example)**:\n",
    "  - Messages were structured as dictionaries with `\"system\"`, `\"user\"`, and `\"AI\"` keys.\n",
    "  - Example:\n",
    "    ```python\n",
    "    messages = [\n",
    "        {\"system\": \"You are a helpful AI assistant.\"},\n",
    "        {\"user\": \"Hello! Can you tell me about LangChain?\"}\n",
    "    ]\n",
    "    ```\n",
    "- **Class-Based (Current Example)**:\n",
    "  - Uses dedicated classes (`SystemMessage`, `HumanMessage`, `AIMessage`) from `langchain_core.messages`.\n",
    "  - Benefits:\n",
    "    - **Type Safety**: Explicit message types (e.g., `HumanMessage` for user inputs).\n",
    "    - **Rich Metadata**: Built-in support for additional attributes (e.g., `name`, `id`).\n",
    "    - **Consistency**: Aligns with LangChain's modular design.\n",
    "\n",
    " Purpose\n",
    "  - Structure multi-turn conversations with `SystemMessage`, `HumanMessage`, and `AIMessage`.\n",
    "  - Retrieve and display the AI's response concisely.\n",
    "  - Transition from dictionary-based to class-based message handling.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"), \n",
    "    HumanMessage(content=\"n8n automation platform\"),\n",
    "    AIMessage(content=\"n8n is a free and open-source workflow automation platform that allows you to create workflows that can be used to automate tasks and processes.\"),\n",
    "    HumanMessage(content=\"how does LSTM work in deep learning?\"),\n",
    "    AIMessage(content=\"LSTM is a type of recurrent neural network that is used to process and analyze sequential data, such as time series or natural language. It is particularly effective for tasks that require long-term dependencies, such as language modeling or speech recognition.\")\n",
    "]\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    response = model.invoke(chat_history)\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "    print(f\"AI: {response.content}\")\n",
    "\n",
    "print(\"---- Message History ----\")\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e201f",
   "metadata": {},
   "source": [
    " Now we understand how can we intialize a langchain LLM and how to add System, Human, and a predefined AI Messages example.\n",
    "\n",
    "### Now lets try a Conversational LangChain chat model with a history\n",
    "\n",
    "This code implements an interactive chat system using LangChain's `ChatOllama` model that maintains full conversation history.\n",
    "\n",
    "Key Features\n",
    "\n",
    "1. **Initial Setup**\n",
    "   ```python\n",
    "   chat_history = [\n",
    "       SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "       HumanMessage(content=\"n8n automation platform\"),\n",
    "       AIMessage(content=\"n8n is a free and open-source...\"),\n",
    "       HumanMessage(content=\"how does LSTM work...\"),\n",
    "       AIMessage(content=\"LSTM is a type of recurrent neural network...\")\n",
    "   ]\n",
    "   ```\n",
    "   - Starts with predefined conversation messages, so we can make sure that the LLM can access these past conversations. you can comment all messages and leave the SystemMessage. try it out yourself!\n",
    "   - Includes system prompt and example Q&A pairs.\n",
    "\n",
    "2. **Interactive Loop**\n",
    "   ```python\n",
    "   while True:\n",
    "       query = input(\"You: \")\n",
    "       if query.lower() == \"exit\":\n",
    "           break\n",
    "       chat_history.append(HumanMessage(content=query))\n",
    "       response = model.invoke(chat_history)\n",
    "       chat_history.append(AIMessage(content=response.content))\n",
    "       print(f\"AI: {response.content}\")\n",
    "   ```\n",
    "   - Takes continuous user input\n",
    "   - Appends each exchange to history\n",
    "   - Maintains context across turns\n",
    "\n",
    "3. **History Tracking**\n",
    "   - Full conversation stored in `chat_history` list\n",
    "   - Each turn adds:\n",
    "     - User's `HumanMessage`\n",
    "     - AI's `AIMessage` response\n",
    "\n",
    "4. **Final Output**\n",
    "   ```python\n",
    "   print(\"---- Message History ----\")\n",
    "   print(chat_history)\n",
    "   ```\n",
    "   - Prints complete conversation at end\n",
    "   - Useful for debugging/analysis\n",
    " \n",
    " How It Works\n",
    "\n",
    "1. The model receives the entire message history each time\n",
    "2. This provides context for more coherent responses\n",
    "3. History grows dynamically during conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2febf",
   "metadata": {},
   "source": [
    "---\n",
    "### Switching LLM Providers in LangChain\n",
    "\n",
    "LangChain makes it simple to switch between different LLM providers. Below are examples using OpenAI and Anthropic instead of Ollama.\n",
    "\n",
    "### 1. OpenAI Provider Example\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Initialize OpenAI chat model\n",
    "model = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# this is it, no need to change anything else.\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Uses `ChatOpenAI` class\n",
    "- Requires `OPENAI_API_KEY` environment variable\n",
    "- Specify model version (e.g., \"gpt-4\")\n",
    "- Same message format as Ollama example\n",
    "\n",
    "### 2. Anthropic Provider Example\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "import os\n",
    "\n",
    "# Set your Anthropic API key\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Initialize Anthropic chat model\n",
    "model = ChatAnthropic(model=\"claude-2\", temperature=0.7)\n",
    "\n",
    "# this is it, no need to change anything else.\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Uses `ChatAnthropic` class\n",
    "- Requires `ANTHROPIC_API_KEY` environment variable\n",
    "- Specify model version (e.g., \"claude-2\")\n",
    "- Identical interface to OpenAI/Ollama versions\n",
    "\n",
    " Comparison Table\n",
    "\n",
    "| Provider  | Class           | Environment Variable    | Example Models     |\n",
    "|-----------|-----------------|-------------------------|--------------------|\n",
    "| OpenAI    | `ChatOpenAI`    | `OPENAI_API_KEY`        | gpt-4, gpt-3.5-turbo |\n",
    "| Anthropic | `ChatAnthropic` | `ANTHROPIC_API_KEY`     | claude-2           |\n",
    "| Ollama    | `ChatOllama`    | (None - local only)     | llama2, mistral    |\n",
    "\n",
    " Key Benefits\n",
    "1. **Consistent Interface**: Same `messages` format and `invoke()` method across providers\n",
    "2. **Easy Switching**: Just change the import and initialization\n",
    "4. **Parameter Consistency**: `temperature` and other parameters work the same way. However, some models can have more/less parameter.\n",
    "\n",
    "- Make sure you install the providers packages.\n",
    "- Set appropriate API keys as environment variables.\n",
    "\n",
    "## Notes\n",
    "- Always check provider documentation for:\n",
    "  - Latest model names\n",
    "  - API key requirements\n",
    "  - Rate limits\n",
    "- Consider cost differences between providers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
